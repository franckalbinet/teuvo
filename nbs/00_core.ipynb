{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> SOM class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from fastcore.all import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "dist_fn = lambda x, ws: np.linalg.norm(ws-x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SOM:\n",
    "    def __init__(self, \n",
    "                 grid_sz:tuple, # grid size\n",
    "                 input_dim:int, # input dimension\n",
    "                 init:str='random', # initialization method\n",
    "                #  dist_fn=np.linalg.norm # distance function\n",
    "                 dist_fn=dist_fn # distance function\n",
    "                 )->None: # initialize the SOM\n",
    "        \"Initialize a Self-Organizing Map with given grid size and input dimension\"\n",
    "        store_attr()\n",
    "        self.weights = None  # Will be initialized when fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _initialize_weights_pca(self:SOM, X:np.ndarray)->np.ndarray:\n",
    "    \"Initialize weights using PCA of the input data\"\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    \n",
    "    # Create grid coordinates and scale by eigenvalues\n",
    "    n = self.grid_sz[0]\n",
    "    alpha = np.linspace(-1, 1, n) * np.sqrt(pca.explained_variance_[0])\n",
    "    beta = np.linspace(-1, 1, n) * np.sqrt(pca.explained_variance_[1])\n",
    "    \n",
    "    # Create the grid\n",
    "    alpha_grid, beta_grid = np.meshgrid(alpha, beta)\n",
    "    \n",
    "    # Initialize weights as linear combination of first two PCs\n",
    "    return (alpha_grid[..., np.newaxis] * pca.components_[0] + \n",
    "            beta_grid[..., np.newaxis] * pca.components_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _initialize_weights(self:SOM, \n",
    "                        X:np.ndarray=None, # data matrix\n",
    "                        method:str='random' # initialization method\n",
    "                        )->np.ndarray: # weights matrix\n",
    "    \"Initialize weights using either random or PCA initialization\"\n",
    "    if method == 'random':\n",
    "        return np.random.randn(*self.grid_sz, self.input_dim)\n",
    "    elif method == 'pca':\n",
    "        if X is None: \n",
    "            raise ValueError(\"Data matrix X required for PCA initialization\")\n",
    "        return self._initialize_weights_pca(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _find_bmu(self:SOM, \n",
    "              x:np.ndarray, # input vector\n",
    "              )->tuple: # coordinates of the best matching unit\n",
    "    \"Find coordinates of Best Matching Unit for input x\"\n",
    "    distances = self.dist_fn(x, self.weights)\n",
    "    return np.unravel_index(np.argmin(distances), self.grid_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _grid_distances(self:SOM, \n",
    "                    bmu_pos:tuple # coordinates of the best matching unit\n",
    "                    )->np.ndarray: # grid distances\n",
    "    \"Calculate grid distances from BMU position\"\n",
    "    rows_idx, cols_idx = np.ogrid[0:self.grid_sz[0], 0:self.grid_sz[1]]\n",
    "    return (bmu_pos[0] - rows_idx)**2 + (bmu_pos[1] - cols_idx)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _neighborhood_function(self:SOM, \n",
    "                           grid_dist:np.ndarray, # grid distances\n",
    "                           sigma:float # neighborhood radius\n",
    "                           )->np.ndarray: # neighborhood function values\n",
    "    \"Calculate neighborhood function values\"\n",
    "    return np.exp(-grid_dist/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _update_weights(self:SOM, \n",
    "                    x:np.ndarray, # input vector\n",
    "                    learning_rate:float, # learning rate\n",
    "                    sigma:float # neighborhood radius\n",
    "                    )->None: # update weights\n",
    "    \"Update weights for a single input vector\"\n",
    "    bmu_pos = self._find_bmu(x)\n",
    "    grid_dist = self._grid_distances(bmu_pos)\n",
    "    neighborhood = self._neighborhood_function(grid_dist, sigma)\n",
    "    self.weights += learning_rate * neighborhood[..., np.newaxis] * (x - self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def exp_sched(start_val, end_val, i, n_steps):\n",
    "    decay = -np.log(end_val/start_val)/n_steps\n",
    "    return start_val * np.exp(-decay * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Scheduler:\n",
    "    def __init__(self, start_val, end_val, step_size, \n",
    "                 n_samples, n_epochs, decay_fn=exp_sched):\n",
    "        store_attr()\n",
    "        self.current_step = 0\n",
    "        self.current_value = start_val\n",
    "        self.total_steps = (n_samples * n_epochs) // step_size\n",
    "    \n",
    "    def step(self, total_samples):\n",
    "        if total_samples % self.step_size == 0:\n",
    "            self.current_value = self.decay_fn(\n",
    "                self.start_val, self.end_val, \n",
    "                self.current_step, self.total_steps\n",
    "            )\n",
    "            self.current_step += 1\n",
    "        return self.current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def fit(self:SOM, \n",
    "        X:np.ndarray, # data matrix\n",
    "        n_epochs:int=20, # number of epochs\n",
    "        lr_scheduler:Scheduler=None, # learning rate scheduler\n",
    "        sigma_scheduler:Scheduler=None, # neighborhood radius scheduler\n",
    "        shuffle:bool=True, # shuffle data\n",
    "        verbose:bool=True # verbose\n",
    "        )->tuple: # tuple of weights, quantization errors, and topographic errors\n",
    "    \"Train the SOM on input data X\"\n",
    "    # Initialize weights if not already done\n",
    "    if self.weights is None:\n",
    "        self.weights = self._initialize_weights(X, self.init)\n",
    "    \n",
    "    # Setup default schedulers if none provided\n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = Scheduler(1.0, 0.01, 100, len(X), n_epochs)\n",
    "    if sigma_scheduler is None:\n",
    "        sigma_scheduler = Scheduler(max(self.grid_sz)/2, 1.0, 100, len(X), n_epochs)\n",
    "    \n",
    "    qe_errors, te_errors = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        X_ = np.random.permutation(X) if shuffle else X.copy()\n",
    "        \n",
    "        # Train on each input vector\n",
    "        for i, x in enumerate(X_):\n",
    "            total_samples = epoch * len(X) + i\n",
    "            lr = lr_scheduler.step(total_samples)\n",
    "            sigma = sigma_scheduler.step(total_samples)\n",
    "            self._update_weights(x, lr, sigma)\n",
    "        \n",
    "        # Calculate errors\n",
    "        qe = self.quantization_error(X)\n",
    "        te = self.topographic_error(X)\n",
    "        qe_errors.append(qe)\n",
    "        te_errors.append(te)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch+1} | QE: {qe:.4f}, TE: {te:.4f}')\n",
    "    \n",
    "    return self.weights, qe_errors, te_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def quantization_error(self:SOM, \n",
    "                       X:np.ndarray # data matrix\n",
    "                       )->float: # quantization error\n",
    "    \"Calculate average distance between each input vector and its BMU\"\n",
    "    return np.array([\n",
    "        self.dist_fn(x, self.weights).min() \n",
    "        for x in X\n",
    "    ]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def topographic_error(self:SOM, \n",
    "                      X:np.ndarray # data matrix\n",
    "                      )->float: # topographic error\n",
    "    \"Calculate proportion of data vectors where 1st and 2nd BMUs are not adjacent\"\n",
    "    def _check_bmu_adjacency(x):\n",
    "        # Get indices of two best matching units\n",
    "        distances = self.dist_fn(x, self.weights)\n",
    "        flat_indices = np.argpartition(distances.flatten(), 2)[:2]\n",
    "        indices = np.unravel_index(flat_indices, self.grid_sz)\n",
    "        # Check if any coordinate differs by more than 1\n",
    "        return any(np.abs(x-y) > 1 for x,y in indices)\n",
    "    \n",
    "    n_errors = sum(_check_bmu_adjacency(x) for x in X)\n",
    "    return 100 * n_errors / len(X)  # Return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def transform(self:SOM, \n",
    "              X:np.ndarray # data matrix\n",
    "              )->np.ndarray: # bmu coordinates\n",
    "    \"Find Best Matching Unit (BMU) coordinates for each input vector\"\n",
    "    bmu_coords = np.zeros((len(X), 2), dtype=int)\n",
    "    for i, x in enumerate(X):\n",
    "        bmu_coords[i] = self._find_bmu(x)\n",
    "    return bmu_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch  \n",
    "def predict(self:SOM, \n",
    "            X:np.ndarray # data matrix\n",
    "            )->np.ndarray: # bmu coordinates\n",
    "    \"Alias for transform method to follow sklearn convention\"\n",
    "    return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch  \n",
    "def _calculate_umatrix(self:SOM)->np.ndarray:\n",
    "    \"Calculate U-Matrix values for current weights\"\n",
    "    def _neighbor_distances(pos):\n",
    "        # Offsets for 8 neighbors\n",
    "        nbr_offsets = [\n",
    "            (-1,-1), (-1,0), (-1,1),  # top-left, top, top-right\n",
    "            (0,-1),          (0,1),   # left, right\n",
    "            (1,-1),  (1,0),  (1,1)    # bottom-left, bottom, bottom-right\n",
    "        ]\n",
    "    \n",
    "        distances = []\n",
    "        weights = []\n",
    "        for dr, dc in nbr_offsets:\n",
    "            r, c = pos\n",
    "            nbr_r, nbr_c = r+dr, c+dc\n",
    "            if (nbr_r >= 0 and nbr_r < self.grid_sz[0] and \n",
    "                nbr_c >= 0 and nbr_c < self.grid_sz[1]):\n",
    "                w = 1/np.sqrt(dr**2 + dc**2)  # weight by distance\n",
    "                weights.append(w)\n",
    "                d = self.dist_fn(self.weights[r,c], self.weights[nbr_r,nbr_c])\n",
    "                distances.append(d)\n",
    "        return np.average(distances, weights=weights)\n",
    "        \n",
    "    umatrix = np.zeros(self.grid_sz)\n",
    "    for i, j in np.ndindex(self.grid_sz):\n",
    "        umatrix[i,j] = _neighbor_distances((i,j))\n",
    "    return umatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def umatrix(self:SOM)->np.ndarray:\n",
    "    \"Return the U-Matrix for current weights\"\n",
    "    return self._calculate_umatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def plot_umatrix(self:SOM, \n",
    "                 figsize=(8,6), \n",
    "                 cmap='viridis_r'):\n",
    "    \"Plot U-Matrix visualization\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(self.umatrix(), cmap=cmap, interpolation='nearest')\n",
    "    plt.colorbar(label='Average distance to neighbors')\n",
    "    plt.title('U-Matrix')\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | QE: 2.1917, TE: 1.8920\n",
      "Epoch: 2 | QE: 2.0051, TE: 2.9494\n",
      "Epoch: 3 | QE: 1.9656, TE: 1.6694\n",
      "Epoch: 4 | QE: 1.8192, TE: 1.1686\n",
      "Epoch: 5 | QE: 1.7921, TE: 1.0017\n",
      "Epoch: 6 | QE: 1.7082, TE: 0.8904\n",
      "Epoch: 7 | QE: 1.6437, TE: 0.5565\n",
      "Epoch: 8 | QE: 1.5971, TE: 1.0017\n",
      "Epoch: 9 | QE: 1.5459, TE: 1.0017\n",
      "Epoch: 10 | QE: 1.4812, TE: 0.5008\n",
      "Epoch: 11 | QE: 1.4391, TE: 0.3339\n",
      "Epoch: 12 | QE: 1.3891, TE: 0.5008\n",
      "Epoch: 13 | QE: 1.3530, TE: 0.3339\n",
      "Epoch: 14 | QE: 1.3136, TE: 0.5008\n",
      "Epoch: 15 | QE: 1.2779, TE: 0.8904\n",
      "Epoch: 16 | QE: 1.2454, TE: 0.7791\n",
      "Epoch: 17 | QE: 1.2159, TE: 0.6121\n",
      "Epoch: 18 | QE: 1.1905, TE: 0.7234\n",
      "Epoch: 19 | QE: 1.1688, TE: 0.7791\n",
      "Epoch: 20 | QE: 1.1502, TE: 0.7791\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Load and normalize MNIST data\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_norm = (X - np.mean(X, axis=-1, keepdims=True))/X.max()\n",
    "\n",
    "# Create and train SOM\n",
    "som = SOM(grid_sz=(20,20), input_dim=64, init='pca')\n",
    "som.fit(X_norm, n_epochs=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
